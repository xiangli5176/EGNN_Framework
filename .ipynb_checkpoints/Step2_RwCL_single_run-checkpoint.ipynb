{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter\n",
    "import yaml\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "from lib_EGNN_Pytorch.models.model_app import RwCL_Model\n",
    "from lib_EGNN_Pytorch import utils, Post_utils, evaluation\n",
    "from lib_EGNN_Pytorch.data_preprocessing import Pre_utils\n",
    "\n",
    "from lib_EGNN_Pytorch.app.RwCL import basic_exec_cluster, basic_exec_link, basic_exec_node_classification\n",
    "from lib_EGNN_Pytorch.app.RwCL import RwCL_app\n",
    "\n",
    "from lib_EGNN_Pytorch.app.RwSL import basic_exec_cluster\n",
    "from lib_EGNN_Pytorch.app.RwSL import RwSL_app\n",
    "# from lib_EGNN_Pytorch.app.RwCL.multi_exec import run_train_cluster, run_test_cluster\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> Setting input data and configurations:\n",
    "tkipf_graph_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data\"\n",
    "\n",
    "data_name = 'cora'\n",
    "# data_name = 'cite'\n",
    "\n",
    "# data_name = 'acm'\n",
    "# data_name = 'dblp'\n",
    "# data_name = 'cite'\n",
    "\n",
    "sdcn_data_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/sdcn/\"\n",
    "workdir = f\"/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/{data_name}/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file_name = f'config_{data_name.lower()}.yaml'\n",
    "config_file_path = os.path.join('./config_data_RwCL_clustering/', config_file_name)\n",
    "with open(config_file_path, 'r') as c:\n",
    "    config = yaml.safe_load(c)    \n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(12345)        \n",
    "        \n",
    "print(\">>>>>>>>>>>>>>>>>>>> Setting Tuning :\")\n",
    "tune_param_name = \"nothing\"\n",
    "# tune_param_name = \"n_epochs\"\n",
    "\n",
    "tune_val_label_list = [1]\n",
    "# tune_val_label_list = [0.0, 0.1, 0.2, 0.5]\n",
    "\n",
    "# tune_val_list = [10**(-val) for val in tune_val_label_list]\n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "# trainer_id_list = [0]\n",
    "trainer_id_list = list(range(1))\n",
    "\n",
    "print(f\"Tune param : {tune_param_name} ; with the following values: {tune_val_list}\")\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>> Loading configs :\")\n",
    "for key, val in config.items():\n",
    "    print(f\"{key}    :    {val}\")\n",
    "    \n",
    "# =================== copy the config file: =======================================\n",
    "dest_folder = os.path.dirname(os.path.join(workdir, f\"tune_{tune_param_name}/\"))\n",
    "if not os.path.exists(os.path.join(dest_folder, config_file_name)):    \n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    shutil.copyfile(config_file_path,  os.path.join(dest_folder, config_file_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tkipf dataset: Cora, PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cython_GBP_data_path = f\"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/{data_name.lower()}/GBP_input/\"\n",
    "# Pre_utils.convert_GBP_input_tkipf_gcn(data_name.lower(), tkipf_graph_path, directed = False,  \n",
    "#                                       normalize = False, redo_save = False)\n",
    "\n",
    "adj_full, features, labels_full, _ = Pre_utils.load_gcn_tkipf_data(tkipf_graph_path, \n",
    "                                                        data_name.lower(), normalize = False, redo_save = False)\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDCN dataset: Cite, ACM, DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cython_GBP_data_path = os.path.join(sdcn_data_path, f\"data_for_GBP_input/{data_name.lower()}/\") \n",
    "Pre_utils.convert_GBP_input_sdcn(data_name, sdcn_data_path, target_path = Cython_GBP_data_path)\n",
    "\n",
    "features, labels_full = Pre_utils.load_sdcn_data_func(sdcn_data_path, data_name)  # both features and labels are numpy array\n",
    "adj_full = Pre_utils.load_sdcn_graph(sdcn_data_path, data_name)  # scipy.sparse.csr_matrix\n",
    "\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n",
    "features_GBP = Pre_utils.GBP_feat_precomputation(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = True,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_GBP = Pre_utils.precompute_Cython_GBP_feat(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = False,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "\n",
    "tune_val_label = tune_val_label_list[0]\n",
    "tune_val = tune_val_list[0]\n",
    "trainer_id = trainer_id_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [features_GBP, labels_full, adj_full]\n",
    "\n",
    "model = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    print(\"ckpt file already exists, so removed ...\")\n",
    "    os.remove(checkpoint_file_path)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(checkpoint_file_path), exist_ok=True)\n",
    "\n",
    "val_metric_path = os.path.join(workdir, \n",
    "                            f\"tune_{tune_param_name}/val_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/val_metric.pkl\")\n",
    "\n",
    "# ==========================  Start the training ==========================\n",
    "time_training, metric_summary = basic_exec_cluster.train(model, config, input_data, device = device, checkpoint_file_path = checkpoint_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Test cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if not os.path.exists(os.path.dirname(checkpoint_file_path)):\n",
    "    raise(\"checkpoint file is missing\")\n",
    "\n",
    "test_metric_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/test_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/test_metric.pkl\")\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> Start test inference\n",
    "test_time, test_metric = basic_exec_cluster.test(model_test, config, input_data, \n",
    "                            device = \"cpu\", checkpoint_file_path = checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train cluster from a class defined from the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = RwCL_app.RwCL_framework(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [features_GBP, labels_full, adj_full]\n",
    "\n",
    "model = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    print(\"ckpt file already exists, so removed ...\")\n",
    "    os.remove(checkpoint_file_path)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(checkpoint_file_path), exist_ok=True)\n",
    "\n",
    "val_metric_path = os.path.join(workdir, \n",
    "                            f\"tune_{tune_param_name}/val_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/val_metric.pkl\")\n",
    "\n",
    "# ==========================  Start the training ==========================\n",
    "time_training, metric_summary = obj.train_cluster(model, config, input_data, device = device, checkpoint_file_path = checkpoint_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if not os.path.exists(os.path.dirname(checkpoint_file_path)):\n",
    "    raise(\"checkpoint file is missing\")\n",
    "\n",
    "test_metric_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/test_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/test_metric.pkl\")\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> Start test inference\n",
    "test_time, test_metric = obj.test_cluster(model_test, config, input_data, \n",
    "                            device = \"cpu\", checkpoint_file_path = checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    \n",
    "    features_GBP = Pre_utils.precompute_Cython_GBP_feat(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = False,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "    \n",
    "    input_data = [features_GBP, labels_full, adj_full]\n",
    "    for trainer_id in trainer_id_list:\n",
    "            print(f\"Training >>> current tuning the hyper-param: {tune_param_name}; with a value : {tune_val} ; on trainer id: {trainer_id}\" )\n",
    "            # encoder = Encoder(config)\n",
    "            model = RwCL_Model(config)\n",
    "            \n",
    "            run_train_cluster(data_name, model, input_data, workdir, config, tune_param_name, tune_val_label, tune_val, \n",
    "                                    trainer_id = trainer_id, device = device)\n",
    "\n",
    "            # test_encoder = Encoder(config)\n",
    "            model_test = RwCL_Model(config)\n",
    "            \n",
    "            run_test_cluster(data_name, model_test, input_data, workdir, config, \n",
    "                                    tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_id_list:\n",
    "        print(f\"Validation Postprocessing >>> current tuning the hyper-param: {tune_param_name}; with a value : {tune_val} ; on trainer id: {trainer_id}\" )\n",
    "        Post_utils.draw_val_metrics(workdir, tune_param_name, tune_val_label, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_utils.generate_val_all_metric(workdir, tune_param_name, tune_val_label_list, tune_val_list, trainer_id_list, real_time=False)\n",
    "\n",
    "Post_utils.generate_val_all_metric(workdir, tune_param_name, tune_val_label_list, tune_val_list, trainer_id_list, real_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_utils.generate_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = [])\n",
    "\n",
    "Post_utils.plot_raw_tune_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = []) \n",
    "\n",
    "Post_utils.plot_stats_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9 )\n",
    "# !(nvidia-smi | grep 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.7]",
   "language": "python",
   "name": "conda-env-pytorch1.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
