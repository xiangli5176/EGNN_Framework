{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter\n",
    "import yaml\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "from lib_EGNN_Pytorch.models.model_app import RwCL_Model\n",
    "from lib_EGNN_Pytorch import utils, Post_utils, evaluation\n",
    "from lib_EGNN_Pytorch.data_preprocessing import Pre_utils\n",
    "\n",
    "from lib_EGNN_Pytorch.app.RwCL.multi_exec import run_train_cluster, run_test_cluster\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>> Setting Tuning :\n",
      "Tune param : nothing ; with the following values: [1]\n",
      ">>>>>>>>>>>>>>>>>>>> Loading configs :\n",
      "data_name    :    cora\n",
      "n_clusters    :    7\n",
      "feat_dim    :    1433\n",
      "seed    :    39788\n",
      "lr    :    0.0001\n",
      "enc_arch    :    256-128\n",
      "mlp_arch    :    256-\n",
      "num_proj_hidden    :    512\n",
      "tau    :    1.0\n",
      "drop_feature_rate_1    :    0.08\n",
      "view_num    :    3\n",
      "num_epochs    :    300\n",
      "weight_decay    :    0.02\n",
      "batch_size_train    :    512\n",
      "eval_display    :    10\n",
      "loss_batch_size    :    0\n",
      "alpha    :    0.1\n",
      "rmax    :    1e-06\n",
      "rrz    :    0.4\n",
      "batchnorm    :    False\n",
      "dropout_rate    :    0.1\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> Setting input data and configurations:\n",
    "tkipf_graph_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data\"\n",
    "\n",
    "data_name = 'cora'\n",
    "# data_name = 'pubmed'\n",
    "# data_name = 'cite'\n",
    "\n",
    "# data_name = 'acm'\n",
    "# data_name = 'dblp'\n",
    "# data_name = 'cite'\n",
    "\n",
    "sdcn_data_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/sdcn/\"\n",
    "workdir = f\"/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/{data_name}/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file_name = f'config_{data_name.lower()}.yaml'\n",
    "config_file_path = os.path.join('./config_data_RwCL_clustering/', config_file_name)\n",
    "with open(config_file_path, 'r') as c:\n",
    "    config = yaml.safe_load(c)    \n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(12345)        \n",
    "        \n",
    "print(\">>>>>>>>>>>>>>>>>>>> Setting Tuning :\")\n",
    "tune_param_name = \"nothing\"\n",
    "# tune_param_name = \"tsne_1\"\n",
    "# tune_param_name = \"dropout_rate\"\n",
    "# tune_param_name = \"n_epochs\"\n",
    "# tune_param_name = \"weight_decay\"\n",
    "\n",
    "tune_val_label_list = [1]\n",
    "# tune_val_label_list = [0.0, 0.1, 0.2, 0.5]\n",
    "\n",
    "# tune_val_list = [10**(-val) for val in tune_val_label_list]\n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "# trainer_id_list = [0]\n",
    "trainer_id_list = list(range(1))\n",
    "\n",
    "print(f\"Tune param : {tune_param_name} ; with the following values: {tune_val_list}\")\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>> Loading configs :\")\n",
    "for key, val in config.items():\n",
    "    print(f\"{key}    :    {val}\")\n",
    "    \n",
    "# =================== copy the config file: =======================================\n",
    "dest_folder = os.path.dirname(os.path.join(workdir, f\"tune_{tune_param_name}/\"))\n",
    "if not os.path.exists(os.path.join(dest_folder, config_file_name)):    \n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    shutil.copyfile(config_file_path,  os.path.join(dest_folder, config_file_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tkipf dataset: Cora, PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed data already exists at: /home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/cora, LOADING...\n"
     ]
    }
   ],
   "source": [
    "Cython_GBP_data_path = f\"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/{data_name.lower()}/GBP_input/\"\n",
    "# Pre_utils.convert_GBP_input_tkipf_gcn(data_name.lower(), tkipf_graph_path, directed = False,  \n",
    "#                                       normalize = False, redo_save = False)\n",
    "\n",
    "adj_full, features, labels_full, _ = Pre_utils.load_gcn_tkipf_data(tkipf_graph_path, \n",
    "                                                        data_name.lower(), normalize = False, redo_save = False)\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDCN dataset: Cite, ACM, DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cython_GBP_data_path = os.path.join(sdcn_data_path, f\"data_for_GBP_input/{data_name.lower()}/\") \n",
    "Pre_utils.convert_GBP_input_sdcn(data_name, sdcn_data_path, target_path = Cython_GBP_data_path)\n",
    "\n",
    "features, labels_full = Pre_utils.load_sdcn_data_func(sdcn_data_path, data_name)  # both features and labels are numpy array\n",
    "adj_full = Pre_utils.load_sdcn_graph(sdcn_data_path, data_name)  # scipy.sparse.csr_matrix\n",
    "\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n",
    "features_GBP = Pre_utils.GBP_feat_precomputation(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = True,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-computation time cost is 39.35373585299385 seconds! \n",
      "Training >>> current tuning the hyper-param: nothing; with a value : 1 ; on trainer id: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'execute_train_investigate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bca08175ecc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRwCL_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             execute_train_investigate(data_name, model, input_data, workdir, config, tune_param_name, tune_val_label, tune_val, \n\u001b[0m\u001b[1;32m     16\u001b[0m                                     trainer_id = trainer_id, device = device)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'execute_train_investigate' is not defined"
     ]
    }
   ],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    \n",
    "    features_GBP = Pre_utils.precompute_Cython_GBP_feat(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = False,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "    \n",
    "    input_data = [features_GBP, labels_full, adj_full]\n",
    "    for trainer_id in trainer_id_list:\n",
    "            print(f\"Training >>> current tuning the hyper-param: {tune_param_name}; with a value : {tune_val} ; on trainer id: {trainer_id}\" )\n",
    "            # encoder = Encoder(config)\n",
    "            model = RwCL_Model(config)\n",
    "            \n",
    "            run_train_cluster(data_name, model, input_data, workdir, config, tune_param_name, tune_val_label, tune_val, \n",
    "                                    trainer_id = trainer_id, device = device)\n",
    "\n",
    "            # test_encoder = Encoder(config)\n",
    "            model_test = RwCL_Model(config)\n",
    "            \n",
    "            run_test_cluster(data_name, model_test, input_data, workdir, config, \n",
    "                                    tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_id_list:\n",
    "        print(f\"Validation Postprocessing >>> current tuning the hyper-param: {tune_param_name}; with a value : {tune_val} ; on trainer id: {trainer_id}\" )\n",
    "        Post_utils.draw_val_metrics(workdir, tune_param_name, tune_val_label, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_utils.generate_val_all_metric(workdir, tune_param_name, tune_val_label_list, tune_val_list, trainer_id_list, real_time=False)\n",
    "\n",
    "Post_utils.generate_val_all_metric(workdir, tune_param_name, tune_val_label_list, tune_val_list, trainer_id_list, real_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_utils.generate_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = [])\n",
    "\n",
    "Post_utils.plot_raw_tune_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = []) \n",
    "\n",
    "Post_utils.plot_stats_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9 )\n",
    "# !(nvidia-smi | grep 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.7]",
   "language": "python",
   "name": "conda-env-pytorch1.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
