{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from lib_EGNN_Pytorch import utils\n",
    "from lib_EGNN_Pytorch.data_preprocessing import Pre_utils_graphsaint\n",
    "from lib_EGNN_Pytorch.app.meta_clustergcn import base_exec_graphsage\n",
    "from lib_EGNN_Pytorch.meta_learn import batch_machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load data settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/xiangli/projects/tmpdata/GCN/GraphSaint/'\n",
    "\n",
    "# read the total epoch number from the yml file to determine the mini_epoch_num and eval_train_every\n",
    "data_name = 'flickr'\n",
    "train_config_yml = './clustergcn_config/flickr2_e.yml'\n",
    "multilabel_tag = False\n",
    "\n",
    "# data_name = 'PPI_small'\n",
    "# train_config_yml = './table2/ppi2_e.yml'\n",
    "\n",
    "working_dir = os.path.join('/home/xiangli/projects/GCN_program/Workshop_local/Meta_Learn_GCN/',\n",
    "                              \"graphsage_res/\",\n",
    "                               data_name.lower()\n",
    "                          )\n",
    "\n",
    "prepare_data_folder = os.path.join(working_dir, 'prepare_data/')\n",
    "img_path = os.path.join(working_dir, 'res_images/')\n",
    "\n",
    "core_par_sampler = 1\n",
    "samples_per_processor = -(-200 // core_par_sampler) # round up division\n",
    "eval_train_every = 5  # period to record the train loss\n",
    "\n",
    "### ================ Start to do flexible settings according to different dataset: \n",
    "\n",
    "\n",
    "tune_param_name = 'mini_epoch_num'\n",
    "tune_val_label_list = [1, 5] \n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "snapshot_period = 2   # period when to take a snapshot of the model for validation later\n",
    "\n",
    "# refer to the yml file to decide the training period:\n",
    "model_epoch_list = list(range(snapshot_period, 16, snapshot_period))    # snapshot epoch list for validation\n",
    "\n",
    "trainer_list = list(range(1))\n",
    "\n",
    "tune_val_label = tune_val_label_list[0]\n",
    "tune_val = tune_val_list[0]\n",
    "trainer_id = trainer_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/meta_learn_GCN/PC_src_code/all_in_one/lib_meta_learn_Pytorch/batch_machine.py:413: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  train_config = yaml.load(f_train_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "Done loading training data..\n"
     ]
    }
   ],
   "source": [
    "# =============== Step1 *** prepare for the batches, models, model_evaluation\n",
    "train_params, train_phases, train_data, arch_gcn = Pre_utils_graphsaint.train_setting(data_name.lower(), datapath, train_config_yml)\n",
    "\n",
    "base_exec_graphsage.prepare(working_dir, train_data, train_params, arch_gcn)\n",
    "train_phase_file_name = os.path.join(prepare_data_folder, 'model_train_phase.pkl')\n",
    "\n",
    "utils.save_info_use_dill(train_phase_file_name, train_phases, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mSTART PHASE    0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/meta_learn_GCN/PC_src_code/all_in_one/lib_meta_learn_Pytorch/samplers.py:61: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.adj_train_norm = scipy.sparse.dia_matrix((1 / self.deg_train,0),shape=adj_train.shape).dot(adj_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 200 subgraphs:   time = 0.867 sec\n",
      "\u001b[1mEpoch    1, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    1, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    2, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    3, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    4, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    5, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch    6, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    7, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    8, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch    9, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   10, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 0\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 1\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 2\u001b[0m\n",
      "\u001b[1mEpoch   11, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   12, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   13, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   14, Batch ID 3\u001b[0m\n",
      "\u001b[1mEpoch   15, Batch ID 3\u001b[0m\n",
      "\u001b[93mOptimization Finished!\u001b[0m\n",
      "\u001b[91mTotal training time: 2680.76 ms\u001b[0m\n",
      "\u001b[91mTotal train data uploading time:  31.28 ms\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============== Step2 *** conduct the training process\n",
    "train_input_file_name = os.path.join(prepare_data_folder, 'model_train_input.pkl') \n",
    "with open(train_input_file_name, \"rb\") as fp:\n",
    "    minibatch, model = dill.load(fp)\n",
    "\n",
    "train_phase_file_name = os.path.join(prepare_data_folder, 'model_train_phase.pkl')\n",
    "with open(train_phase_file_name, \"rb\") as fp:\n",
    "    train_phases = dill.load(fp)\n",
    "\n",
    "\n",
    "### ==========================  single run of training\n",
    "tune_model_folder = os.path.join(working_dir, 'model_snapshot', f'tune_{tune_param_name}_{tune_val_label}',\n",
    "                        f'model_trainer_{trainer_id}/')\n",
    "    \n",
    "os.makedirs(tune_model_folder, exist_ok=True)\n",
    "\n",
    "# to apply any tuning values\n",
    "total_time_train, time_upload = base_exec_graphsage.train_investigate(tune_model_folder, train_phases, model, minibatch, eval_train_every, \n",
    "                                snapshot_every = snapshot_period, mini_epoch_num = 5, multilabel = multilabel_tag, \n",
    "                          core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)    \n",
    "\n",
    "\n",
    "\n",
    "# for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "#     for trainer_id in trainer_list:\n",
    "#         execute_train_investigate(img_path, working_dir, train_phases, model, minibatch, eval_train_every, \n",
    "#                                   tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id,\n",
    "#                                   snapshot_every = snapshot_period, mini_epoch_num = 5, multilabel = multilabel_tag, input_neigh_deg = [10, 5],\n",
    "#                                   core_par_sampler = core_par_sampler, samples_per_processor = samples_per_processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n",
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    }
   ],
   "source": [
    "# ================ Step3*** investigate validation:\n",
    "evaluation_input_file_name = os.path.join(prepare_data_folder, 'model_eval_input.pkl')\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "\n",
    "\n",
    "# perform the evaluation\n",
    "tune_model_folder = os.path.join(working_dir, 'model_snapshot', f'tune_{tune_param_name}_{tune_val_label}',\n",
    "                                    f'model_trainer_{trainer_id}/')\n",
    "\n",
    "validation_res_folder = os.path.join(img_path, 'validation_res', f'tune_{tune_param_name}_{tune_val_label}',\n",
    "                                    f'validation_trainer_{trainer_id}/')\n",
    "os.makedirs(validation_res_folder, exist_ok=True)\n",
    "\n",
    "# start evaluation:\n",
    "for validation_epoch in model_epoch_list:\n",
    "    res = batch_machine.evaluate(tune_model_folder, minibatch_eval, model_eval, validation_epoch)    \n",
    "    validation_res_file_name = os.path.join(validation_res_folder, f'model_epoch_{validation_epoch}')\n",
    "    with open(validation_res_file_name, \"wb\") as fp:\n",
    "        pickle.dump(res, fp)\n",
    "    \n",
    "# for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "#     for trainer_id in trainer_list:\n",
    "#         execute_validation_investigate(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "#                                 tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during the evaluation step, report the input matrices size: \n",
      "adj_subgraph size :  torch.Size([89250, 89250]); \t feat_subg size : torch.Size([89250, 500])\n"
     ]
    }
   ],
   "source": [
    "# ================= Step4*** investigate test:\n",
    "evaluation_input_file_name = os.path.join(prepare_data_folder, 'model_eval_input.pkl')\n",
    "with open(evaluation_input_file_name, \"rb\") as fp:\n",
    "    minibatch_eval, model_eval = dill.load(fp)\n",
    "    \n",
    "\n",
    "### ======================== Perform the single run of the test =======================\n",
    "f1mic_best, ep_best = 0, -1\n",
    "validation_res_folder = os.path.join(img_path, 'validation_res', f'tune_{tune_param_name}_{tune_val_label}',\n",
    "                                f'validation_trainer_{trainer_id}/')\n",
    "\n",
    "\n",
    "for validation_epoch in model_epoch_list:\n",
    "    validation_res_file_name = os.path.join(validation_res_folder, f'model_epoch_{validation_epoch}')\n",
    "    with open(validation_res_file_name, \"rb\") as fp:\n",
    "        f1mic_val, f1mac_val = pickle.load(fp)\n",
    "\n",
    "    if f1mic_val > f1mic_best:\n",
    "        f1mic_best, ep_best = f1mic_val, validation_epoch\n",
    "\n",
    "# use the selected model to perform on the test\n",
    "tune_model_folder = os.path.join(working_dir, 'model_snapshot', f'tune_{tune_param_name}_{tune_val_label}',\n",
    "                                f'model_trainer_{trainer_id}/')\n",
    "\n",
    "# return 1) micro-f1 ;  2) macro-f1\n",
    "res = batch_machine.evaluate(tune_model_folder, minibatch_eval, model_eval, ep_best, mode = 'test')        \n",
    "\n",
    "\n",
    "# for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "#     for trainer_id in trainer_list:\n",
    "#         execute_test_tuning(img_path, working_dir, minibatch_eval, model_eval, model_epoch_list, \n",
    "#                                 tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_1_4_geometric]",
   "language": "python",
   "name": "conda-env-pytorch_1_4_geometric-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
