{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter\n",
    "import yaml\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "from lib_EGNN_Pytorch.models.model_app import RwCL_Model\n",
    "from lib_EGNN_Pytorch import utils, Post_utils, evaluation\n",
    "from lib_EGNN_Pytorch.data_preprocessing import Pre_utils\n",
    "\n",
    "from lib_EGNN_Pytorch.app.RwCL.multi_exec import run_train_cluster, run_test_cluster\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>> Setting Tuning :\n",
      "Tune param : nothing ; with the following values: [1]\n",
      ">>>>>>>>>>>>>>>>>>>> Loading configs :\n",
      "data_name    :    cora\n",
      "n_clusters    :    7\n",
      "feat_dim    :    1433\n",
      "seed    :    39788\n",
      "lr    :    0.0001\n",
      "enc_arch    :    256-128\n",
      "mlp_arch    :    256-\n",
      "num_proj_hidden    :    512\n",
      "tau    :    1.0\n",
      "drop_feature_rate_1    :    0.08\n",
      "view_num    :    3\n",
      "num_epochs    :    300\n",
      "weight_decay    :    0.02\n",
      "batch_size_train    :    512\n",
      "eval_display    :    10\n",
      "loss_batch_size    :    0\n",
      "alpha    :    0.1\n",
      "rmax    :    1e-06\n",
      "rrz    :    0.4\n",
      "batchnorm    :    False\n",
      "dropout_rate    :    0.1\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> Setting input data and configurations:\n",
    "tkipf_graph_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data\"\n",
    "\n",
    "data_name = 'cora'\n",
    "# data_name = 'pubmed'\n",
    "# data_name = 'cite'\n",
    "\n",
    "# data_name = 'acm'\n",
    "# data_name = 'dblp'\n",
    "# data_name = 'cite'\n",
    "\n",
    "sdcn_data_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/sdcn/\"\n",
    "workdir = f\"/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/{data_name}/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file_name = f'config_{data_name.lower()}.yaml'\n",
    "config_file_path = os.path.join('./config_data_RwCL_clustering/', config_file_name)\n",
    "with open(config_file_path, 'r') as c:\n",
    "    config = yaml.safe_load(c)    \n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(12345)        \n",
    "        \n",
    "print(\">>>>>>>>>>>>>>>>>>>> Setting Tuning :\")\n",
    "tune_param_name = \"nothing\"\n",
    "# tune_param_name = \"tsne_1\"\n",
    "# tune_param_name = \"dropout_rate\"\n",
    "# tune_param_name = \"n_epochs\"\n",
    "# tune_param_name = \"weight_decay\"\n",
    "\n",
    "tune_val_label_list = [1]\n",
    "# tune_val_label_list = [0.0, 0.1, 0.2, 0.5]\n",
    "\n",
    "# tune_val_list = [10**(-val) for val in tune_val_label_list]\n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "# trainer_id_list = [0]\n",
    "trainer_id_list = list(range(1))\n",
    "\n",
    "print(f\"Tune param : {tune_param_name} ; with the following values: {tune_val_list}\")\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>> Loading configs :\")\n",
    "for key, val in config.items():\n",
    "    print(f\"{key}    :    {val}\")\n",
    "    \n",
    "# =================== copy the config file: =======================================\n",
    "dest_folder = os.path.dirname(os.path.join(workdir, f\"tune_{tune_param_name}/\"))\n",
    "if not os.path.exists(os.path.join(dest_folder, config_file_name)):    \n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    shutil.copyfile(config_file_path,  os.path.join(dest_folder, config_file_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tkipf dataset: Cora, PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed data already exists at: /home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/cora, LOADING...\n"
     ]
    }
   ],
   "source": [
    "Cython_GBP_data_path = f\"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/{data_name.lower()}/GBP_input/\"\n",
    "# Pre_utils.convert_GBP_input_tkipf_gcn(data_name.lower(), tkipf_graph_path, directed = False,  \n",
    "#                                       normalize = False, redo_save = False)\n",
    "\n",
    "adj_full, features, labels_full, _ = Pre_utils.load_gcn_tkipf_data(tkipf_graph_path, \n",
    "                                                        data_name.lower(), normalize = False, redo_save = False)\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDCN dataset: Cite, ACM, DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cython_GBP_data_path = os.path.join(sdcn_data_path, f\"data_for_GBP_input/{data_name.lower()}/\") \n",
    "Pre_utils.convert_GBP_input_sdcn(data_name, sdcn_data_path, target_path = Cython_GBP_data_path)\n",
    "\n",
    "features, labels_full = Pre_utils.load_sdcn_data_func(sdcn_data_path, data_name)  # both features and labels are numpy array\n",
    "adj_full = Pre_utils.load_sdcn_graph(sdcn_data_path, data_name)  # scipy.sparse.csr_matrix\n",
    "\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n",
    "features_GBP = Pre_utils.GBP_feat_precomputation(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = True,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-computation time cost is 38.385678304002795 seconds! \n",
      "Training >>> current tuning the hyper-param: nothing; with a value : 1 ; on trainer id: 0\n",
      "ckpt file already exists, so removed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 17:04:06 (INFO): Epoch   10 | total train loss: 6.388 | Trained local batch number: 4 | train time: 0.658s\n",
      "2022-04-20 17:04:07 (INFO): Epoch   20 | total train loss: 6.308 | Trained local batch number: 2 | train time: 0.866s\n",
      "2022-04-20 17:04:07 (INFO): Epoch   30 | total train loss: 4.975 | Trained local batch number: 6 | train time: 1.066s\n",
      "2022-04-20 17:04:08 (INFO): Epoch   40 | total train loss: 6.224 | Trained local batch number: 4 | train time: 1.266s\n",
      "2022-04-20 17:04:09 (INFO): Epoch   50 | total train loss: 6.205 | Trained local batch number: 2 | train time: 1.477s\n",
      "2022-04-20 17:04:09 (INFO): Epoch   60 | total train loss: 4.897 | Trained local batch number: 6 | train time: 1.684s\n",
      "2022-04-20 17:04:10 (INFO): Epoch   70 | total train loss: 6.166 | Trained local batch number: 4 | train time: 1.888s\n",
      "2022-04-20 17:04:11 (INFO): Epoch   80 | total train loss: 6.162 | Trained local batch number: 2 | train time: 2.092s\n",
      "2022-04-20 17:04:12 (INFO): Epoch   90 | total train loss: 4.925 | Trained local batch number: 6 | train time: 2.299s\n",
      "2022-04-20 17:04:12 (INFO): Epoch  100 | total train loss: 6.155 | Trained local batch number: 4 | train time: 2.507s\n",
      "2022-04-20 17:04:13 (INFO): Epoch  110 | total train loss: 6.127 | Trained local batch number: 2 | train time: 2.720s\n",
      "2022-04-20 17:04:14 (INFO): Epoch  120 | total train loss: 4.874 | Trained local batch number: 6 | train time: 2.931s\n",
      "2022-04-20 17:04:14 (INFO): Epoch  130 | total train loss: 6.110 | Trained local batch number: 4 | train time: 3.137s\n",
      "2022-04-20 17:04:15 (INFO): Epoch  140 | total train loss: 6.093 | Trained local batch number: 2 | train time: 3.347s\n",
      "2022-04-20 17:04:16 (INFO): Epoch  150 | total train loss: 4.853 | Trained local batch number: 6 | train time: 3.547s\n",
      "2022-04-20 17:04:16 (INFO): Epoch  160 | total train loss: 6.095 | Trained local batch number: 4 | train time: 3.756s\n",
      "2022-04-20 17:04:17 (INFO): Epoch  170 | total train loss: 6.075 | Trained local batch number: 2 | train time: 3.970s\n",
      "2022-04-20 17:04:18 (INFO): Epoch  180 | total train loss: 4.854 | Trained local batch number: 6 | train time: 4.180s\n",
      "2022-04-20 17:04:19 (INFO): Epoch  190 | total train loss: 6.087 | Trained local batch number: 4 | train time: 4.393s\n",
      "2022-04-20 17:04:19 (INFO): Epoch  200 | total train loss: 6.095 | Trained local batch number: 2 | train time: 4.595s\n",
      "2022-04-20 17:04:20 (INFO): Epoch  210 | total train loss: 4.831 | Trained local batch number: 6 | train time: 4.803s\n",
      "2022-04-20 17:04:21 (INFO): Epoch  220 | total train loss: 6.075 | Trained local batch number: 4 | train time: 5.013s\n",
      "2022-04-20 17:04:22 (INFO): Epoch  230 | total train loss: 6.078 | Trained local batch number: 2 | train time: 5.226s\n",
      "2022-04-20 17:04:22 (INFO): Epoch  240 | total train loss: 4.844 | Trained local batch number: 6 | train time: 5.433s\n",
      "2022-04-20 17:04:23 (INFO): Epoch  250 | total train loss: 6.075 | Trained local batch number: 4 | train time: 5.643s\n",
      "2022-04-20 17:04:24 (INFO): Epoch  260 | total train loss: 6.070 | Trained local batch number: 2 | train time: 5.858s\n",
      "2022-04-20 17:04:24 (INFO): Epoch  270 | total train loss: 4.832 | Trained local batch number: 6 | train time: 6.066s\n",
      "2022-04-20 17:04:25 (INFO): Epoch  280 | total train loss: 6.069 | Trained local batch number: 4 | train time: 6.277s\n",
      "2022-04-20 17:04:26 (INFO): Epoch  290 | total train loss: 6.056 | Trained local batch number: 2 | train time: 6.489s\n",
      "2022-04-20 17:04:26 (INFO): Epoch  300 | total train loss: 4.810 | Trained local batch number: 6 | train time: 6.698s\n",
      "2022-04-20 17:04:27 (INFO): Training task (tune_param_name: nothing; tune_val: 1; trainer_id: 0) take: 6.698 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/cora/tune_nothing/val_metric/tunelabel_1_trainer_0/val_metric.pkl, path folder already exists, so removed ...\n",
      "/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/cora/tune_nothing/train_profile/tunelabel_1_trainer_0/train_profile.pkl, path folder already exists, so removed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 17:04:27 (INFO): Test metrics: | Accuracy : 0.697562776957164 | f1_micro : 0.697562776957164 | f1_macro : 0.67864007512008 | NMI : 0.5314883187429671 | ARI : 0.4682034784320485 | conductance : 0.09852216748768473 | modularity : 0.7435804043208233 | test time: 0.195s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Runtime: 0.19s\n",
      "/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/cora/tune_nothing/test_metric/tunelabel_1_trainer_0/test_metric.pkl, path folder already exists, so removed ...\n",
      "/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/cora/tune_nothing/test_profile/tunelabel_1_trainer_0/test_profile.pkl, path folder already exists, so removed ...\n"
     ]
    }
   ],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    \n",
    "    features_GBP = Pre_utils.precompute_Cython_GBP_feat(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = False,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "    \n",
    "    input_data = [features_GBP, labels_full, adj_full]\n",
    "    for trainer_id in trainer_id_list:\n",
    "            print(f\"Training >>> current tuning the hyper-param: {tune_param_name}; with a value : {tune_val} ; on trainer id: {trainer_id}\" )\n",
    "            # encoder = Encoder(config)\n",
    "            model = RwCL_Model(config)\n",
    "            \n",
    "            run_train_cluster(data_name, model, input_data, workdir, config, tune_param_name, tune_val_label, tune_val, \n",
    "                                    trainer_id = trainer_id, device = device)\n",
    "\n",
    "            # test_encoder = Encoder(config)\n",
    "            model_test = RwCL_Model(config)\n",
    "            \n",
    "            run_test_cluster(data_name, model_test, input_data, workdir, config, \n",
    "                                    tune_param_name, tune_val_label, tune_val, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tune_val_label, tune_val in zip(tune_val_label_list, tune_val_list):\n",
    "    for trainer_id in trainer_id_list:\n",
    "        print(f\"Validation Postprocessing >>> current tuning the hyper-param: {tune_param_name}; with a value : {tune_val} ; on trainer id: {trainer_id}\" )\n",
    "        Post_utils.draw_val_metrics(workdir, tune_param_name, tune_val_label, trainer_id = trainer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_utils.generate_val_all_metric(workdir, tune_param_name, tune_val_label_list, tune_val_list, trainer_id_list, real_time=False)\n",
    "\n",
    "Post_utils.generate_val_all_metric(workdir, tune_param_name, tune_val_label_list, tune_val_list, trainer_id_list, real_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post_utils.generate_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = [])\n",
    "\n",
    "Post_utils.plot_raw_tune_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = []) \n",
    "\n",
    "Post_utils.plot_stats_test_table(workdir, tune_param_name, \n",
    "                    tune_val_label_list, tune_val_list, trainer_id_list, skip_trainer = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9 )\n",
    "# !(nvidia-smi | grep 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.7]",
   "language": "python",
   "name": "conda-env-pytorch1.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
