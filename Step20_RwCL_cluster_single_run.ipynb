{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter\n",
    "import yaml\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "from lib_EGNN_Pytorch.models.model_app import RwCL_Model\n",
    "from lib_EGNN_Pytorch import utils, evaluation\n",
    "from lib_EGNN_Pytorch.data_preprocessing import Pre_utils, GBP_precompute\n",
    "\n",
    "from lib_EGNN_Pytorch.app.RwCL import basic_exec_cluster  #, basic_exec_link, basic_exec_node_classification\n",
    "from lib_EGNN_Pytorch.app.RwCL import RwCL_app\n",
    "\n",
    "# from lib_EGNN_Pytorch.app.RwSL import basic_exec_cluster\n",
    "# from lib_EGNN_Pytorch.app.RwSL import RwSL_app\n",
    "# from lib_EGNN_Pytorch.app.RwCL.multi_exec import run_train_cluster, run_test_cluster\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>> Setting Tuning :\n",
      "Tune param : nothing ; with the following values: [1]\n",
      ">>>>>>>>>>>>>>>>>>>> Loading configs :\n",
      "data_name    :    cora\n",
      "n_clusters    :    7\n",
      "feat_dim    :    1433\n",
      "seed    :    39788\n",
      "lr    :    0.0001\n",
      "enc_arch    :    256-128\n",
      "mlp_arch    :    256-\n",
      "num_proj_hidden    :    512\n",
      "tau    :    1.0\n",
      "drop_feature_rate_1    :    0.08\n",
      "view_num    :    3\n",
      "num_epochs    :    300\n",
      "weight_decay    :    0.02\n",
      "batch_size_train    :    512\n",
      "eval_display    :    10\n",
      "loss_batch_size    :    0\n",
      "alpha    :    0.1\n",
      "rmax    :    1e-06\n",
      "rrz    :    0.4\n",
      "batchnorm    :    False\n",
      "dropout_rate    :    0.1\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> Setting input data and configurations:\n",
    "tkipf_graph_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data\"\n",
    "\n",
    "data_name = 'cora'\n",
    "# data_name = 'cite'\n",
    "\n",
    "sdcn_data_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/sdcn/\"\n",
    "workdir = f\"/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/{data_name}/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file_name = f'config_{data_name.lower()}.yaml'\n",
    "config_file_path = os.path.join('./config_data_RwCL_clustering/', config_file_name)\n",
    "with open(config_file_path, 'r') as c:\n",
    "    config = yaml.safe_load(c)    \n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(12345)        \n",
    "        \n",
    "print(\">>>>>>>>>>>>>>>>>>>> Setting Tuning :\")\n",
    "tune_param_name = \"nothing\"\n",
    "# tune_param_name = \"n_epochs\"\n",
    "\n",
    "tune_val_label_list = [1]\n",
    "# tune_val_label_list = [0.0, 0.1, 0.2, 0.5]\n",
    "\n",
    "# tune_val_list = [10**(-val) for val in tune_val_label_list]\n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "# trainer_id_list = [0]\n",
    "trainer_id_list = list(range(1))\n",
    "\n",
    "print(f\"Tune param : {tune_param_name} ; with the following values: {tune_val_list}\")\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>> Loading configs :\")\n",
    "for key, val in config.items():\n",
    "    print(f\"{key}    :    {val}\")\n",
    "    \n",
    "# =================== copy the config file: =======================================\n",
    "dest_folder = os.path.dirname(os.path.join(workdir, f\"tune_{tune_param_name}/\"))\n",
    "if not os.path.exists(os.path.join(dest_folder, config_file_name)):    \n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    shutil.copyfile(config_file_path,  os.path.join(dest_folder, config_file_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tkipf dataset: Cora, PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed data already exists at: /home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/cora, LOADING...\n"
     ]
    }
   ],
   "source": [
    "Cython_GBP_data_path = f\"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/{data_name.lower()}/GBP_input/\"\n",
    "\n",
    "adj_full, features, labels_full, _ = Pre_utils.load_gcn_tkipf_data(tkipf_graph_path, \n",
    "                                                        data_name.lower(), normalize = False, redo_save = False)\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-computation time cost is 35.52190001300187 seconds! \n"
     ]
    }
   ],
   "source": [
    "features_GBP = GBP_precompute.precompute_Cython_GBP_feat(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = False,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "\n",
    "tune_val_label = tune_val_label_list[0]\n",
    "tune_val = tune_val_list[0]\n",
    "trainer_id = trainer_id_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt file already exists, so removed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 18:21:03 (INFO): Epoch   10 | total train loss: 6.349 | Trained local batch number: 4 | train time: 0.680s\n",
      "2022-04-21 18:21:04 (INFO): Epoch   20 | total train loss: 6.278 | Trained local batch number: 2 | train time: 0.869s\n",
      "2022-04-21 18:21:05 (INFO): Epoch   30 | total train loss: 4.974 | Trained local batch number: 6 | train time: 1.066s\n",
      "2022-04-21 18:21:06 (INFO): Epoch   40 | total train loss: 6.229 | Trained local batch number: 4 | train time: 1.270s\n",
      "2022-04-21 18:21:06 (INFO): Epoch   50 | total train loss: 6.215 | Trained local batch number: 2 | train time: 1.479s\n",
      "2022-04-21 18:21:07 (INFO): Epoch   60 | total train loss: 4.900 | Trained local batch number: 6 | train time: 1.672s\n",
      "2022-04-21 18:21:07 (INFO): Epoch   70 | total train loss: 6.169 | Trained local batch number: 4 | train time: 1.862s\n",
      "2022-04-21 18:21:08 (INFO): Epoch   80 | total train loss: 6.165 | Trained local batch number: 2 | train time: 2.071s\n",
      "2022-04-21 18:21:09 (INFO): Epoch   90 | total train loss: 4.908 | Trained local batch number: 6 | train time: 2.264s\n",
      "2022-04-21 18:21:09 (INFO): Epoch  100 | total train loss: 6.162 | Trained local batch number: 4 | train time: 2.458s\n",
      "2022-04-21 18:21:10 (INFO): Epoch  110 | total train loss: 6.120 | Trained local batch number: 2 | train time: 2.648s\n",
      "2022-04-21 18:21:10 (INFO): Epoch  120 | total train loss: 4.871 | Trained local batch number: 6 | train time: 2.845s\n",
      "2022-04-21 18:21:11 (INFO): Epoch  130 | total train loss: 6.116 | Trained local batch number: 4 | train time: 3.039s\n",
      "2022-04-21 18:21:12 (INFO): Epoch  140 | total train loss: 6.099 | Trained local batch number: 2 | train time: 3.259s\n",
      "2022-04-21 18:21:12 (INFO): Epoch  150 | total train loss: 4.856 | Trained local batch number: 6 | train time: 3.461s\n",
      "2022-04-21 18:21:13 (INFO): Epoch  160 | total train loss: 6.101 | Trained local batch number: 4 | train time: 3.664s\n",
      "2022-04-21 18:21:14 (INFO): Epoch  170 | total train loss: 6.073 | Trained local batch number: 2 | train time: 3.855s\n",
      "2022-04-21 18:21:14 (INFO): Epoch  180 | total train loss: 4.843 | Trained local batch number: 6 | train time: 4.045s\n",
      "2022-04-21 18:21:15 (INFO): Epoch  190 | total train loss: 6.088 | Trained local batch number: 4 | train time: 4.233s\n",
      "2022-04-21 18:21:15 (INFO): Epoch  200 | total train loss: 6.090 | Trained local batch number: 2 | train time: 4.425s\n",
      "2022-04-21 18:21:16 (INFO): Epoch  210 | total train loss: 4.838 | Trained local batch number: 6 | train time: 4.616s\n",
      "2022-04-21 18:21:16 (INFO): Epoch  220 | total train loss: 6.079 | Trained local batch number: 4 | train time: 4.811s\n",
      "2022-04-21 18:21:17 (INFO): Epoch  230 | total train loss: 6.068 | Trained local batch number: 2 | train time: 5.003s\n",
      "2022-04-21 18:21:18 (INFO): Epoch  240 | total train loss: 4.848 | Trained local batch number: 6 | train time: 5.194s\n",
      "2022-04-21 18:21:18 (INFO): Epoch  250 | total train loss: 6.076 | Trained local batch number: 4 | train time: 5.386s\n",
      "2022-04-21 18:21:19 (INFO): Epoch  260 | total train loss: 6.070 | Trained local batch number: 2 | train time: 5.578s\n",
      "2022-04-21 18:21:19 (INFO): Epoch  270 | total train loss: 4.830 | Trained local batch number: 6 | train time: 5.766s\n",
      "2022-04-21 18:21:20 (INFO): Epoch  280 | total train loss: 6.068 | Trained local batch number: 4 | train time: 5.955s\n",
      "2022-04-21 18:21:20 (INFO): Epoch  290 | total train loss: 6.053 | Trained local batch number: 2 | train time: 6.144s\n",
      "2022-04-21 18:21:21 (INFO): Epoch  300 | total train loss: 4.815 | Trained local batch number: 6 | train time: 6.333s\n"
     ]
    }
   ],
   "source": [
    "input_data = [features_GBP, labels_full, adj_full]\n",
    "\n",
    "model = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    print(\"ckpt file already exists, so removed ...\")\n",
    "    os.remove(checkpoint_file_path)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(checkpoint_file_path), exist_ok=True)\n",
    "\n",
    "val_metric_path = os.path.join(workdir, \n",
    "                            f\"tune_{tune_param_name}/val_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/val_metric.pkl\")\n",
    "\n",
    "# ==========================  Start the training ==========================\n",
    "time_training, metric_summary = basic_exec_cluster.train(model, config, input_data, device = device, checkpoint_file_path = checkpoint_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Test cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 18:21:22 (INFO): Test metrics: | Accuracy : 0.7031019202363368 | f1_micro : 0.7031019202363368 | f1_macro : 0.6841359619200945 | NMI : 0.5318728744405148 | ARI : 0.4780424695780298 | conductance : 0.0975748389541493 | modularity : 0.7456871064673785 | test time: 0.230s\n"
     ]
    }
   ],
   "source": [
    "model_test = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if not os.path.exists(os.path.dirname(checkpoint_file_path)):\n",
    "    raise(\"checkpoint file is missing\")\n",
    "\n",
    "test_metric_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/test_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/test_metric.pkl\")\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> Start test inference\n",
    "test_time, test_metric = basic_exec_cluster.test(model_test, config, input_data, \n",
    "                            device = \"cpu\", checkpoint_file_path = checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train cluster from a class defined from the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = RwCL_app.RwCL_framework(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt file already exists, so removed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 18:21:23 (INFO): Epoch   10 | total train loss: 6.407 | Trained local batch number: 4 | train time: 0.190s\n",
      "2022-04-21 18:21:23 (INFO): Epoch   20 | total train loss: 6.262 | Trained local batch number: 2 | train time: 0.379s\n",
      "2022-04-21 18:21:24 (INFO): Epoch   30 | total train loss: 4.988 | Trained local batch number: 6 | train time: 0.572s\n",
      "2022-04-21 18:21:24 (INFO): Epoch   40 | total train loss: 6.230 | Trained local batch number: 4 | train time: 0.761s\n",
      "2022-04-21 18:21:25 (INFO): Epoch   50 | total train loss: 6.190 | Trained local batch number: 2 | train time: 0.975s\n",
      "2022-04-21 18:21:26 (INFO): Epoch   60 | total train loss: 4.939 | Trained local batch number: 6 | train time: 1.164s\n",
      "2022-04-21 18:21:26 (INFO): Epoch   70 | total train loss: 6.178 | Trained local batch number: 4 | train time: 1.354s\n",
      "2022-04-21 18:21:27 (INFO): Epoch   80 | total train loss: 6.178 | Trained local batch number: 2 | train time: 1.544s\n",
      "2022-04-21 18:21:27 (INFO): Epoch   90 | total train loss: 4.889 | Trained local batch number: 6 | train time: 1.732s\n",
      "2022-04-21 18:21:28 (INFO): Epoch  100 | total train loss: 6.140 | Trained local batch number: 4 | train time: 1.926s\n",
      "2022-04-21 18:21:29 (INFO): Epoch  110 | total train loss: 6.114 | Trained local batch number: 2 | train time: 2.117s\n",
      "2022-04-21 18:21:29 (INFO): Epoch  120 | total train loss: 4.853 | Trained local batch number: 6 | train time: 2.310s\n",
      "2022-04-21 18:21:30 (INFO): Epoch  130 | total train loss: 6.083 | Trained local batch number: 4 | train time: 2.501s\n",
      "2022-04-21 18:21:30 (INFO): Epoch  140 | total train loss: 6.114 | Trained local batch number: 2 | train time: 2.693s\n",
      "2022-04-21 18:21:31 (INFO): Epoch  150 | total train loss: 4.875 | Trained local batch number: 6 | train time: 2.885s\n",
      "2022-04-21 18:21:32 (INFO): Epoch  160 | total train loss: 6.106 | Trained local batch number: 4 | train time: 3.075s\n",
      "2022-04-21 18:21:32 (INFO): Epoch  170 | total train loss: 6.076 | Trained local batch number: 2 | train time: 3.265s\n",
      "2022-04-21 18:21:33 (INFO): Epoch  180 | total train loss: 4.859 | Trained local batch number: 6 | train time: 3.454s\n",
      "2022-04-21 18:21:33 (INFO): Epoch  190 | total train loss: 6.075 | Trained local batch number: 4 | train time: 3.643s\n",
      "2022-04-21 18:21:34 (INFO): Epoch  200 | total train loss: 6.087 | Trained local batch number: 2 | train time: 3.831s\n",
      "2022-04-21 18:21:35 (INFO): Epoch  210 | total train loss: 4.856 | Trained local batch number: 6 | train time: 4.019s\n",
      "2022-04-21 18:21:35 (INFO): Epoch  220 | total train loss: 6.084 | Trained local batch number: 4 | train time: 4.209s\n",
      "2022-04-21 18:21:36 (INFO): Epoch  230 | total train loss: 6.066 | Trained local batch number: 2 | train time: 4.398s\n",
      "2022-04-21 18:21:36 (INFO): Epoch  240 | total train loss: 4.827 | Trained local batch number: 6 | train time: 4.586s\n",
      "2022-04-21 18:21:37 (INFO): Epoch  250 | total train loss: 6.057 | Trained local batch number: 4 | train time: 4.774s\n",
      "2022-04-21 18:21:38 (INFO): Epoch  260 | total train loss: 6.074 | Trained local batch number: 2 | train time: 4.963s\n",
      "2022-04-21 18:21:38 (INFO): Epoch  270 | total train loss: 4.834 | Trained local batch number: 6 | train time: 5.151s\n",
      "2022-04-21 18:21:39 (INFO): Epoch  280 | total train loss: 6.065 | Trained local batch number: 4 | train time: 5.342s\n",
      "2022-04-21 18:21:39 (INFO): Epoch  290 | total train loss: 6.066 | Trained local batch number: 2 | train time: 5.533s\n",
      "2022-04-21 18:21:40 (INFO): Epoch  300 | total train loss: 4.834 | Trained local batch number: 6 | train time: 5.724s\n"
     ]
    }
   ],
   "source": [
    "input_data = [features_GBP, labels_full, adj_full]\n",
    "\n",
    "model = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    print(\"ckpt file already exists, so removed ...\")\n",
    "    os.remove(checkpoint_file_path)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(checkpoint_file_path), exist_ok=True)\n",
    "\n",
    "val_metric_path = os.path.join(workdir, \n",
    "                            f\"tune_{tune_param_name}/val_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/val_metric.pkl\")\n",
    "\n",
    "# ==========================  Start the training ==========================\n",
    "time_training, metric_summary = obj.train_cluster(model, config, input_data, device = device, checkpoint_file_path = checkpoint_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 18:21:40 (INFO): Test metrics: | Accuracy : 0.6580502215657311 | f1_micro : 0.6580502215657311 | f1_macro : 0.6477307457394293 | NMI : 0.5099481151457238 | ARI : 0.42946472166956956 | conductance : 0.10515346722243274 | modularity : 0.7417936723479576 | test time: 0.136s\n"
     ]
    }
   ],
   "source": [
    "model_test = RwCL_Model(config)\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if not os.path.exists(os.path.dirname(checkpoint_file_path)):\n",
    "    raise(\"checkpoint file is missing\")\n",
    "\n",
    "test_metric_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/test_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/test_metric.pkl\")\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> Start test inference\n",
    "test_time, test_metric = obj.test_cluster(model_test, config, input_data, \n",
    "                            device = \"cpu\", checkpoint_file_path = checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9 )\n",
    "# !(nvidia-smi | grep 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.7]",
   "language": "python",
   "name": "conda-env-pytorch1.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
