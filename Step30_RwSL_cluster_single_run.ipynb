{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import os.path as osp\n",
    "import random\n",
    "from time import perf_counter\n",
    "import yaml\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "from lib_EGNN_Pytorch.models.GNN_basic import AE\n",
    "from lib_EGNN_Pytorch.models.model_app import RwSL_Model, set_dims_RwSL\n",
    "from lib_EGNN_Pytorch import utils, evaluation\n",
    "from lib_EGNN_Pytorch.data_preprocessing import Pre_utils, GBP_precompute\n",
    "\n",
    "from lib_EGNN_Pytorch.app.RwSL import basic_exec_cluster\n",
    "from lib_EGNN_Pytorch.app.RwSL import RwSL_app\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>> Setting Tuning :\n",
      "Tune param : nothing ; with the following values: [1]\n",
      ">>>>>>>>>>>>>>>>>>>> Loading configs :\n",
      "name    :    cora\n",
      "seed    :    20159\n",
      "lr    :    0.0001\n",
      "k    :    None\n",
      "n_clusters    :    7\n",
      "n_input    :    1433\n",
      "batch_size_update_p    :    512\n",
      "arch    :    512-1024-16\n",
      "golden_pre_epoch    :    10\n",
      "golden_trainer    :    3\n",
      "dropout_rate    :    0.0\n",
      "bn_momentum    :    0.1\n",
      "weight_decay    :    0.01\n",
      "batch_size_train    :    256\n",
      "v    :    1.0\n",
      "a    :    0.1\n",
      "b    :    0.01\n",
      "sigma    :    0.5\n",
      "n_epochs    :    60\n",
      "alpha    :    0.1\n",
      "rmax    :    1e-05\n",
      "rrz    :    0.4\n",
      "batchnorm    :    True\n",
      "update_p    :    1\n",
      "eval_step    :    5\n",
      "display_eval_step    :    20\n",
      "batch_size_pre_train    :    256\n",
      "pretrain_lr    :    0.0001\n",
      "pretrain_n_epochs    :    30\n",
      "pretrain_weight_decay    :    0.01\n",
      "pretrain_eval_step    :    5\n",
      "early_stop    :    False\n",
      "patience    :    40\n",
      "stop_standard    :    modularity\n",
      "pretrain_path    :    /home/xiangli/projects/tmpdata/GCN/Graph_Clustering/sdcn/pretrain_model_parameter/GBP_self_pretrain/pretrain_info_cora_batchnorm/cora.pkl\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>> Setting input data and configurations:\n",
    "tkipf_graph_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data\"\n",
    "sdcn_data_path = \"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/sdcn/\"\n",
    "\n",
    "data_name = 'cora'\n",
    "# data_name = 'cite'\n",
    "\n",
    "\n",
    "workdir = f\"/home/xiangli/projects/GCN_program/Workshop_local/EGNN_workdir_results/RwSL/{data_name}/cluster/\"\n",
    "\n",
    "pretrain_save_path = os.path.join(sdcn_data_path, f'pretrain_model_parameter/GBP_self_pretrain/pretrain_info_{data_name}_batchnorm/{data_name}.pkl')\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config_file_name = f'config_{data_name.lower()}.yaml'\n",
    "config_file_path = os.path.join('./config_data_RwSL/', config_file_name)\n",
    "with open(config_file_path, 'r') as c:\n",
    "    config = yaml.safe_load(c)    \n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "\n",
    "torch.manual_seed(config['seed'])\n",
    "random.seed(12345)        \n",
    "        \n",
    "print(\">>>>>>>>>>>>>>>>>>>> Setting Tuning :\")\n",
    "tune_param_name = \"nothing\"\n",
    "# tune_param_name = \"n_epochs\"\n",
    "\n",
    "tune_val_label_list = [1]\n",
    "# tune_val_label_list = [0.0, 0.1, 0.2, 0.5]\n",
    "\n",
    "# tune_val_list = [10**(-val) for val in tune_val_label_list]\n",
    "tune_val_list = [val for val in tune_val_label_list]\n",
    "\n",
    "# trainer_id_list = [0]\n",
    "trainer_id_list = list(range(1))\n",
    "\n",
    "config[\"pretrain_path\"] = pretrain_save_path\n",
    "\n",
    "print(f\"Tune param : {tune_param_name} ; with the following values: {tune_val_list}\")\n",
    "print(\">>>>>>>>>>>>>>>>>>>> Loading configs :\")\n",
    "for key, val in config.items():\n",
    "    print(f\"{key}    :    {val}\")\n",
    "    \n",
    "# =================== copy the config file: =======================================\n",
    "dest_folder = os.path.dirname(os.path.join(workdir, f\"tune_{tune_param_name}/\"))\n",
    "if not os.path.exists(os.path.join(dest_folder, config_file_name)):    \n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    shutil.copyfile(config_file_path,  os.path.join(dest_folder, config_file_name))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tkipf dataset: Cora, PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed data already exists at: /home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/cora, LOADING...\n"
     ]
    }
   ],
   "source": [
    "Cython_GBP_data_path = f\"/home/xiangli/projects/tmpdata/GCN/Graph_Clustering/tkipf_gcn_data/Packed_data/no_row_normalize/{data_name.lower()}/GBP_input/\"\n",
    "# Pre_utils.convert_GBP_input_tkipf_gcn(data_name.lower(), tkipf_graph_path, directed = False,  \n",
    "#                                       normalize = False, redo_save = False)\n",
    "\n",
    "adj_full, features, labels_full, _ = Pre_utils.load_gcn_tkipf_data(tkipf_graph_path, \n",
    "                                                        data_name.lower(), normalize = False, redo_save = False)\n",
    "\n",
    "features = np.ascontiguousarray(features, dtype = np.float32)\n",
    "adj_matrix_cython = np.ascontiguousarray(np.load(os.path.join(Cython_GBP_data_path, f'{data_name.lower()}_adj.npy')), dtype=np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pre-computation time cost is 3.238839063999876 seconds! \n"
     ]
    }
   ],
   "source": [
    "features_GBP = GBP_precompute.precompute_Cython_GBP_feat(data_name, 40, \n",
    "                                config[\"alpha\"], config[\"rmax\"], config[\"rrz\"], \n",
    "                                rwnum = 0, directed = False, add_self_loop = False,\n",
    "                                rand_seed = 10, \n",
    "                                feats = features, adj_matrix = adj_matrix_cython)\n",
    "\n",
    "tune_val_label = tune_val_label_list[0]\n",
    "tune_val = tune_val_list[0]\n",
    "trainer_id = trainer_id_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pre-train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dims, dec_dims, _ = set_dims_RwSL(config)\n",
    "pretrain_model = AE(enc_dims, dec_dims, config)\n",
    "\n",
    "# train_time, loss_hist = basic_exec_cluster.pretrain_ae(pretrain_model, config, features_GBP, \n",
    "#                                     device = device, pretrain_save_path = pretrain_save_path)\n",
    "\n",
    "# Post_utils.draw_pretrain_AE_loss(pretrain_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt file already exists, so removed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 20:20:11 (INFO): Epoch   20 | total train loss: 0.023 | train time: 10.503s\n",
      "2022-04-24 20:20:21 (INFO): Epoch   40 | total train loss: 0.020 | train time: 20.399s\n",
      "2022-04-24 20:20:32 (INFO): Epoch   60 | total train loss: 0.020 | train time: 30.761s\n"
     ]
    }
   ],
   "source": [
    "input_data = [features_GBP, labels_full, adj_full]\n",
    "\n",
    "model_train = RwSL_Model(config,\n",
    "               n_clusters=config[\"n_clusters\"],\n",
    "                v=1.0, \n",
    "                pretrain_path = config[\"pretrain_path\"])\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    print(\"ckpt file already exists, so removed ...\")\n",
    "    os.remove(checkpoint_file_path)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(checkpoint_file_path), exist_ok=True)\n",
    "\n",
    "# val_metric_path = os.path.join(workdir, \n",
    "#                             f\"tune_{tune_param_name}/val_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/val_metric.pkl\")\n",
    "\n",
    "# ==========================  Start the training ==========================\n",
    "time_training, metric_summary = basic_exec_cluster.train(model_train, config, input_data, device = device, checkpoint_file_path = checkpoint_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Test cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 20:20:46 (INFO): Test metrics: | Accuracy : 0.2518463810930576 | f1_micro : 0.2518463810930576 | f1_macro : 0.11426784241525585 | NMI : 0.0669474395726579 | ARI : -0.0206928271325985 | conductance : 0.046419098143236075 | modularity : 0.14337201717152318 | test time: 0.276s\n"
     ]
    }
   ],
   "source": [
    "model_test = RwSL_Model(config,\n",
    "               n_clusters=config[\"n_clusters\"],\n",
    "                v=1.0, \n",
    "                pretrain_path = config[\"pretrain_path\"])\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if not os.path.exists(os.path.dirname(checkpoint_file_path)):\n",
    "    raise(\"checkpoint file is missing\")\n",
    "\n",
    "test_metric_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/test_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/test_metric.pkl\")\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> Start test inference\n",
    "test_time, test_metric = basic_exec_cluster.test(model_test, config, input_data, \n",
    "                            device = \"cpu\", checkpoint_file_path = checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform train cluster from a class defined from the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = RwSL_app.RwSL_framework(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 21:10:20 (INFO): Epoch   20 | total train loss: 0.021 | train time: 18.603s\n",
      "2022-04-21 21:10:39 (INFO): Epoch   40 | total train loss: 0.018 | train time: 36.816s\n",
      "2022-04-21 21:10:58 (INFO): Epoch   60 | total train loss: 0.019 | train time: 55.316s\n"
     ]
    }
   ],
   "source": [
    "input_data = [features_GBP, labels_full, adj_full]\n",
    "\n",
    "model_train = RwSL_Model(config,\n",
    "               n_clusters=config[\"n_clusters\"],\n",
    "                v=1.0, \n",
    "                pretrain_path = config[\"pretrain_path\"])\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "    print(\"ckpt file already exists, so removed ...\")\n",
    "    os.remove(checkpoint_file_path)\n",
    "else:\n",
    "    os.makedirs(os.path.dirname(checkpoint_file_path), exist_ok=True)\n",
    "\n",
    "val_metric_path = os.path.join(workdir, \n",
    "                            f\"tune_{tune_param_name}/val_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/val_metric.pkl\")\n",
    "\n",
    "# ==========================  Start the training ==========================\n",
    "time_training, metric_summary = obj.train_cluster(model_train, config, input_data, device = device, checkpoint_file_path = checkpoint_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 21:10:59 (INFO): Test metrics: | Accuracy : 0.275480059084195 | f1_micro : 0.275480059084195 | f1_macro : 0.12129655408589715 | NMI : 0.05696476380259752 | ARI : -0.012091254821594465 | conductance : 0.05229253505115574 | modularity : 0.16199088181030144 | test time: 0.171s\n"
     ]
    }
   ],
   "source": [
    "model_test = RwSL_Model(config,\n",
    "               n_clusters=config[\"n_clusters\"],\n",
    "                v=1.0, \n",
    "                pretrain_path = config[\"pretrain_path\"])\n",
    "\n",
    "checkpoint_file_path = os.path.join(workdir,\n",
    "                f\"tune_{tune_param_name}/model_checkpoint/tunelabel_{tune_val_label}_trainer_{trainer_id}/best_model.pkl\")\n",
    "\n",
    "if not os.path.exists(os.path.dirname(checkpoint_file_path)):\n",
    "    raise(\"checkpoint file is missing\")\n",
    "\n",
    "# test_metric_path = os.path.join(workdir,\n",
    "#                 f\"tune_{tune_param_name}/test_metric/tunelabel_{tune_val_label}_trainer_{trainer_id}/test_metric.pkl\")\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> Start test inference\n",
    "test_time, test_metric = obj.test_cluster(model_test, config, input_data, \n",
    "                            device = \"cpu\", checkpoint_file_path = checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU flush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # free GPU memory\n",
    "# !(nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9 )\n",
    "# !(nvidia-smi | grep 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.7]",
   "language": "python",
   "name": "conda-env-pytorch1.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
